---
title: "Predictive Inference for Cognitive Decline Using Mixed Effects Models: Detailed Analysis"
author: "Daniel Dema"
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 6
    fig_height: 4
    dev: 'svg'
    self_contained: false
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = getwd()) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(spida2)
library(p3d)
library(nlme)
library(car)
library(lattice)
library(latticeExtra)
library(dplyr)
library(glmmTMB)
library(DHARMa)
library(splines)
library(performance)
library(lme4)
library(lmerTest)
```

# Introduction

This document provides the details for the construction of the models presented in the report, focusing primarily on the analysis of interaction terms and diagnostics for random effects and residuals. This document is written under the assumption that it is being read with the report readily available; details about the dataset and references are omitted to minimize repetition of the context established in the report.

# Data Loading and Cleaning

Note that in the original dataset, SES is ranked backwards, in the sense that 1 is the highest and 5 is the lowest. When loading the data, we reverse the order for ease of interpretability.

```{r}
df <- read.csv("D:/Daniel/Documents/MATH6642/final_project/Data/oasis_longitudinal.csv")
dc <- df[!is.na(df$MMSE) & !is.na(df$SES), ]

dc$SES <- 5 - dc$SES

dc_orig <- dc

table_per_subject <- table(dc$Subject.ID)
table_per_subject[table_per_subject < 2]

dc <- dc[order(dc$Subject.ID, dc$Age), ]
```

# Data visualization

```{r}
xqplot(dc)

par(mfrow = c(1, 3))

hist(dc$EDUC, 
     breaks = 10, 
     main = "Histogram of Education Level", 
     xlab = "Years of Education",
     ylab = "Number of Individuals",
     col = "lightblue",
     border = "white")

barplot(table(dc$SES),
        main = "Barplot of Socioeconomic Status",
        xlab = "SES",
        ylab = "Number of Individuals",
        col = "lightblue",
        border = "white")

barplot(table(dc$M.F), 
        main = "Bar Chart of Sex", 
        xlab = "Sex", 
        ylab = "Number of Individuals", 
        col = c("pink", "lightblue"))

par(mfrow = c(1, 1))

initial_visits <- dc[ave(dc$Visit, dc$Subject.ID, FUN = min) == dc$Visit, ]
final_visits <- dc[ave(dc$Visit, dc$Subject.ID, FUN = max) == dc$Visit, ]

#Set layout: 1 row, 2 columns
par(mfrow = c(1, 2))

#Histogram for initial visits
hist(initial_visits$MMSE,
     main = "Initial Visit MMSE", 
     breaks = 0:30,
     xlab = "MMSE",
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

#Histogram for final visits
hist(final_visits$MMSE,
     main = "Final Visit MMSE", 
     breaks = 0:30,
     xlab = "MMSE",
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

#Reset layout
par(mfrow = c(1, 1))

#Get initial visits per subject
initial_visits <- dc %>%
  group_by(Subject.ID) %>%
  filter(Visit == min(Visit)) %>%
  ungroup()

#Filter subjects who had CDR = 0 at initial visit
subjects_cdr0 <- initial_visits %>%
  filter(CDR == 0) %>%
  pull(Subject.ID)

#Initial visits for subjects with CDR = 0 at initial visit
initial_cdr0 <- dc %>%
  filter(Subject.ID %in% subjects_cdr0) %>%
  group_by(Subject.ID) %>%
  filter(Visit == min(Visit)) %>%
  ungroup()

#Final visits for subjects with CDR = 0 at initial visit
final_cdr0 <- dc %>%
  filter(Subject.ID %in% subjects_cdr0) %>%
  group_by(Subject.ID) %>%
  filter(Visit == max(Visit)) %>%
  ungroup()

par(mfrow = c(1, 2))

hist(initial_cdr0$MMSE, 
     breaks = 0:30, 
     main = "Initial Visit MMSE (CDR=0)", 
     xlab = "MMSE", 
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

hist(final_cdr0$MMSE, 
     breaks = 0:30, 
     main = "Final Visit MMSE (CDR=0)", 
     xlab = "MMSE", 
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

par(mfrow = c(1, 1))

#Get initial visits per subject
initial_visits <- dc %>%
  group_by(Subject.ID) %>%
  filter(Visit == min(Visit)) %>%
  ungroup()

#Filter subjects who had CDR = 0.5 at initial visit
subjects_cdr05 <- initial_visits %>%
  filter(CDR == 0.5) %>%
  pull(Subject.ID)

#Initial visits for subjects with CDR = 0.5 at initial visit
initial_cdr05 <- dc %>%
  filter(Subject.ID %in% subjects_cdr05) %>%
  group_by(Subject.ID) %>%
  filter(Visit == min(Visit)) %>%
  ungroup()

#Final visits for subjects with CDR = 0.5 at initial visit
final_cdr05 <- dc %>%
  filter(Subject.ID %in% subjects_cdr05) %>%
  group_by(Subject.ID) %>%
  filter(Visit == max(Visit)) %>%
  ungroup()

par(mfrow = c(1, 2))

#Histogram for initial visits
hist(initial_cdr05$MMSE, 
     breaks = 0:30, 
     main = "Initial Visit MMSE (CDR=0.5)", 
     xlab = "MMSE", 
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

#Histogram for final visits
hist(final_cdr05$MMSE, 
     breaks = 0:30, 
     main = "Final Visit MMSE (CDR=0.5)", 
     xlab = "MMSE", 
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

par(mfrow = c(1, 1))

#Get initial visits per subject
initial_visits <- dc %>%
  group_by(Subject.ID) %>%
  filter(Visit == min(Visit)) %>%
  ungroup()

#Filter subjects who had CDR = 1 at initial visit
subjects_cdr1 <- initial_visits %>%
  filter(CDR == 1) %>%
  pull(Subject.ID)

#Initial visits for subjects with CDR = 1 at initial visit
initial_cdr1 <- dc %>%
  filter(Subject.ID %in% subjects_cdr1) %>%
  group_by(Subject.ID) %>%
  filter(Visit == min(Visit)) %>%
  ungroup()

#Final visits for subjects with CDR = 1 at initial visit
final_cdr1 <- dc %>%
  filter(Subject.ID %in% subjects_cdr1) %>%
  group_by(Subject.ID) %>%
  filter(Visit == max(Visit)) %>%
  ungroup()

par(mfrow = c(1, 2))

#Histogram for initial visits
hist(initial_cdr1$MMSE, 
     breaks = 0:30, 
     main = "Initial Visit MMSE (CDR=1)", 
     xlab = "MMSE", 
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

#Histogram for final visits
hist(final_cdr1$MMSE, 
     breaks = 0:30, 
     main = "Final Visit MMSE (CDR=1)", 
     xlab = "MMSE", 
     ylab = "Number of Individuals",
     col = "lightblue", 
     border = "white", 
     xlim = c(0, 30))

par(mfrow = c(1, 1))
```

We see immediately from the xqplot that MMSE is heavily left-skewed, the data has slightly more females than males, and socioeconomic status tends to the higher end. Histograms and bar plots are attached for ease of visualization, and we can hypothesize from inspecting the MMSE histograms stratified by CDR that the overall decline in MMSE is in majority due to the decline in the groups CDR = 0.5 and CDR = 1. 

# Longitudinal Plots

We recreate the by subject nWBV trajectories originally presented in (Marcus, Fotenos, et al., 2010), as well as the analogous MMSE trajectories. We compute the mean of the slopes of the MMSE trajectories.

```{r}
cdr_colors <- c("0" = "lightblue", "0.5" = "violet", "1" = "darkblue")

# nWBV trajectories
xyplot(nWBV ~ Age, data = dc,
       groups = Subject.ID,
       type = "b",
       lwd = 1,
       pch = 16,
       col = cdr_colors[as.character(dc$CDR)],
       xlab = "Age (years)",
       ylab = "nWBV (%)",
       main = "Longitudinal trajectories of nWBV per subject by CDR group",
       key = list(text = list(c("CDR 0", "CDR 0.5", "CDR 1")),
                  points = list(pch = 16, col = c("lightblue", "violet", "darkblue")),
                  columns = 3))

# MMSE trajectories
xyplot(MMSE ~ Age, data = dc,
       groups = Subject.ID,
       type = "b",
       lwd = 1,
       pch = 16,
       col = cdr_colors[as.character(dc$CDR)],
       xlab = "Age (years)",
       ylab = "MMSE",
       main = "Longitudinal trajectories of MMSE per subject by CDR group",
       key = list(text = list(c("CDR 0", "CDR 0.5", "CDR 1")),
                  points = list(pch = 16, col = c("lightblue", "violet", "darkblue")),
                  columns = 3))
                  
# Copy dc to avoid modifying original
dc_copy <- dc

# Calculate slopes per subject manually
subject_slopes <- dc_copy %>%
  group_by(Subject.ID, CDR) %>%
  filter(n() > 1) %>%  # need at least two points per subject
  summarise(
    slope = {
      fit <- lm(MMSE ~ Age, data = cur_data())
      coef(fit)["Age"]
    },
    .groups = "drop"
  )

# Average slopes by CDR group
avg_slopes_by_CDR_old <- subject_slopes %>%
  group_by(CDR) %>%
  summarise(
    avg_slope = mean(slope, na.rm = TRUE),
    n = n()
  )

print(avg_slopes_by_CDR_old)
```

Group CDR = 0 has mean MMSE slope -0.0306, group CDR = 0.5 has mean MMSE slope -0.464, and group CDR = 1 has mean MMSE slope -0.826.

# Linear Mixed Effects Modelling

```{r}
# We begin with considering random effects. As a preliminary model, we include each predictor in the fixed effects and
# consider Age as a random effect since it's time-variant.

fit1 <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC, 
                  data = dc, 
                  random = ~ 1 + Age | Subject.ID,
                  correlation = corAR1(form = ~ 1 | Subject.ID))
summary(fit1)

# We will compare it to the same fixed effects but with just a random intercept.

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC, 
            data = dc, 
            random = ~ 1 | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID))
summary(test)

anova(fit1, test)

# Since the AIC is lower and p-value is high, we proceed without the random effect of Age.

fit1 <- update(fit1, random = ~ 1 | Subject.ID)

# nWBV is also time-variant, so we will test it as a random effect.

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# We see that the AIC is much lower and p-value is very low, so we proceed by including the random effect of nWBV.

fit2 <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(fit2)

# Now we consider interaction terms. We consider three possible 3-way Interaction terms:

# 1) nWBV * Age * SES: SES influences access to healthcare, slowing decay in
#nWBV as the subject ages.

# 2) nWBV * Age * M.F: Possible differences in nWBV decay by sex as the subject ages.

# 3) nWBV * Age * EDUC: Higher education slows the loss of brain volume as the subject ages.

# Case 1)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * Age * SES
            + nWBV * Age + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

test_ml <- update(test, method = "ML")
fit2_ml <- update(fit2, method = "ML")

anova(test_ml, fit2_ml)

# The test model as a higher AIC with low p-value, so it appears to perform better. But it's worth noting that all
# coefficients have p-values > 0.05. We will give this model a proper name and return to it later.

fit3 <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * Age * SES
            + nWBV * Age + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(fit3)

# Case 2)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * Age * SES
            + nWBV * Age + nWBV * SES + Age * SES + nWBV * Age * M.F
            + nWBV * M.F + Age * M.F, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:
test_ml <- update(test, method = "ML")
fit3_ml <- update(fit3, method = "ML")

anova(test_ml, fit3_ml)

# The AIC for this model is higher than our previous interaction model, and the p-value is quite high. We will discard this model # and consider the next case.

# Case 3)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * Age * SES
            + nWBV * Age + nWBV * SES + Age * SES + nWBV * Age * EDUC
            + nWBV * EDUC + Age * EDUC, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:
test_ml <- update(test, method = "ML")

anova(test_ml, fit3_ml)

# We again see a higher AIC and high p-value, so we discard this new model.


# Although our first interaction model performs relatively well in terms of AIC, as noted earlier, all coefficients are 
# statistically insignificant (p-values > 0.05.) We will discard this model and instead add 2-way interaction terms to
# our original model one by one. We consider the following cases:

# 1) nWBV * Age : Brain volume changes with age.

# 2) nWBV * EDUC: Education acts as a buffer against brain volume loss.

# 3) nWBV * SES : Higher SES allows for better healthcare, improving preservation of brain volume.

# 4) nWBV * M.F : Sex affects brain volume.

# 5) Age * EDUC : Education acts as a buffer against age effects.

# 6) Age * M.F : Aging affects women and men differently.

# 7) Age * SES : Higher SES acts as a buffer against age effects by again allowing for better healthcare.

# Case 1)

dc$nWBV <- scale(dc$nWBV, center = TRUE, scale = FALSE)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * Age, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# We will compare this model to the same model with centered age in the random effects.

test2 <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * Age, 
            data = dc, 
            random = ~ 1 + dvar(nWBV, Subject.ID) | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test2)

anova(test, test2)

# By AIC comparison, centering appears to worsen the model performance.

# switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
ml_fit2 <- update(fit2, method = "ML")
anova(ml_fit2, ml_test)

# The p-value is high and the AIC increases comparing this model to our original model; we discard this interaction and proceed
# with the next one.

# Case 2)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * EDUC, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
anova(ml_fit2, ml_test)

# There is very slight improvement in AIC but the p-value is high; we discard this interaction and consider the next one.

# Case 3)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
anova(ml_fit2, ml_test)

# We see an improvement in AIC with a low p-value, so we will keep this interaction. We give this model a proper name and
# consider the next interaction.

fit4 <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))

# Case 4)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * SES + nWBV * M.F, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
ml_fit4 <- update(fit4, method = "ML")
anova(ml_fit4, ml_test)

# There is minimal improvement in AIC with a high p-value. We discard this interaction and proceed with the next one.

# Case 5)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * SES + Age * EDUC, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
anova(ml_fit4, ml_test)

# Again, there is minimal improvement in AIC with a high p-value. We discard this interaction and proceed with the next one.

#Case 6)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * SES + Age * M.F, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
anova(ml_fit4, ml_test)

# Once again, there is minimal improvement in AIC with a high p-value. We discard this interaction and proceed with the next one.

#Case 7)

test <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
anova(ml_fit4, ml_test)

# We see a decrease in AIC with a low p-value, so we will keep this interaction term. We give this model a name.

fit5 <- lme(MMSE ~ nWBV + Age + M.F + SES + EDUC + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(fit5)

# Notice that, unlike in our earlier models, many coefficients in this model are statistically significant. Since sex and
# education appear to be insignificant, we will see if dropping them improves the model, starting with sex:

test <- lme(MMSE ~ nWBV + Age + SES + EDUC + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

ml_fit5 <- update(fit5, method = "ML")
ml_test <- update(test, method = "ML")
anova(ml_fit5, ml_test)

# Since the p-value is high with minimal difference in AIC, we will drop sex. Now we see if we can drop education:

fit5 <- lme(MMSE ~ nWBV + Age + SES + EDUC + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

#Switch to ML so we can use ANOVA:

ml_test <- update(test, method = "ML")
ml_fit5 <- update(fit5, method ="ML")
anova(ml_fit5, ml_test)

# Again, there is minimal difference in AIC with a high p-value, so we drop education.

fit5 <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(fit5)

ml_fit5 <- update(fit5, method = "ML")

# We compare our latest model with our two earlier models using an ANOVA test:

anova(fit2, fit3, fit5)

# We see that our latest model has the lowest AIC and all terms are statistically significant, so we proceed with this model
# as our tentative "best" model.

dc$fit_vals2 <- fitted(fit2)
dc$fit_vals3 <- fitted(fit3)
dc$fit_vals5 <- fitted(fit5)
```

# Diagnostics for Random Effects

```{r}
summary(fit5)

# Notice that the random effects matrix is numerically unstable due to the correlation being very close to -1 (-0.999),
# which suggests that the random slope and intercept are almost perfectly negatively correlated. We will attempt to
# resolve this by centering the random slope within-subject.

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + dvar(nWBV, Subject.ID) | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

anova(fit5, test)
dc$test <- fitted(test)

# Centering resolves the correlation issue, but at the cost of a relatively higher AIC, as well as the Age * SES interaction term
# losing significance. What if we revisit the idea of dropping random slope altogether?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

anova(fit5, test)

# AIC increases and we have a low p-value, so this suggests we should keep the random slope. What if we instead revisit the idea
# of Age as a random effect?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + Age | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)
anova(fit5, test)

# AIC increases again; moreover, notice that the correlation between slope and intercept becomes very close to 0. Perhaps
# centering Age resolves this?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + dvar(Age, Subject.ID) | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

anova(fit5, test)

# Centering the age again results in a higher AIC; moreover, notice that the correlation is now very close to 1.

# What if we try including both nWBV and Age in random effects?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ nWBV + Age | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

anova(test, fit5)

# For nWBV the correlation is very close to -1 and for Age the correlation is very close to 0. And we again see a higher AIC
# with a high p-value. What if we try centering both terms?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ dvar(nWBV, Subject.ID) + dvar(Age, Subject.ID) | Subject.ID,
            correlation = corAR1(form = ~ 1 | Subject.ID),
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

anova(fit5, test)

# Centering stabilizes the correlations at the cost of a higher AIC.

# What happens if we center nWBV and remove autocorrelation?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + dvar(nWBV, Subject.ID) | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

anova(test, fit5)

# We again see that AIC increases with a small p-value.

# What if we remove autocorrelation from the model with centered nWBV and centered Age?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ dvar(nWBV, Subject.ID) + dvar(Age, Subject.ID) | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)
anova(fit5, test)

# AIC is higher again, but the p-value is small. What if we drop the insignificant Age * SES term?

test <- lme(MMSE ~ nWBV + Age + SES + nWBV * SES, 
            data = dc, 
            random = ~ dvar(nWBV, Subject.ID) + dvar(Age, Subject.ID) | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Switch to ML so we can use ANOVA:

fit5_ml <- update(fit5, method = "ML")
test_ml <- update(test, method = "ML")
anova(fit5_ml, test_ml)

# Again, the AIC is higher. What if we drop nWBV*SES?

test <- lme(MMSE ~ nWBV + Age + SES, 
            data = dc, 
            random = ~ dvar(nWBV, Subject.ID) + dvar(Age, Subject.ID) | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

#ML for anova
test_ml <- update(test, method = "ML")
anova(fit5_ml, test_ml)

# AIC is still higher.

# At this point, our options are a trade-off between lower AIC and an extra interaction term with our earlier model (fit5) versus
# relatively stable random effects with a centered nWBV term. 

# Another possible remedy is to consider incorporating non-linear terms into the model. We will consider this approach in the
# next section on residuals.
```

# Diagnostics for Residuals

```{r}
hist(resid(fit5), main = "Histogram of Residuals (fit5)")
qqnorm(resid(fit5)); qqline(resid(fit5), col = "red")
plot(fit5)

# We see kurtosis in the QQ plot. The standardized residuals plot is not well-suited for longitudinal models; instead, we will
# need to reconstruct our model using glmmTMB so that we can then use DHARMa to plot residuals.

fit5_tmb <- glmmTMB(
  MMSE ~ nWBV + Age + SES + nWBV:SES + Age:SES + (1 + nWBV | Subject.ID),
  data = dc,
  REML = TRUE
)

res <- simulateResiduals(fittedModel = fit5_tmb)
plot(res)

# Failing the KS test indicates that our residuals are not uniformly distributed, and failing the outlier test suggests that
# our model handles outliers poorly. The curves in the residual plot being highly non-linear suggests that the model handles
# heteroskedasticity poorly.

#We will try incoporating non-linear terms into the model. Beginning with a square term for Age:

fit_age_poly <- glmmTMB(
  MMSE ~ poly(Age, 2) + nWBV + SES + nWBV:SES + poly(Age, 2):SES + (1 + nWBV | Subject.ID),
  data = dc,
  REML = TRUE
)

sim_age_poly <- simulateResiduals(fit_age_poly)
plot(sim_age_poly)

# We see no improvement in the residual plots.

dc$nWBV_sq <- dc$nWBV^2
dc$Age_sq <- dc$Age^2

# What if we consider a square term for nWBV?

fit_nwbv_poly_raw <- glmmTMB(
  MMSE ~ nWBV + nWBV_sq + Age + SES + nWBV:SES + nWBV_sq:SES + Age:SES + (1 + nWBV | Subject.ID),
  data = dc,
  REML = TRUE
)

sim_nwbv_poly <- simulateResiduals(fit_nwbv_poly_raw)
plot(sim_nwbv_poly)

# We again see no improvement in the residual plots.

# What if we drop slope from the random effects?

fit_simple_re <- glmmTMB(
  MMSE ~ nWBV + Age + SES + nWBV:SES + Age:SES + (1 | Subject.ID),
  data = dc,
  REML = TRUE
)

# Our deviations get even worse; this validates our earlier tests that suggested keeping the random slope.

# Let's compare AIC values:

sim_simple <- simulateResiduals(fit_simple_re)
plot(sim_simple)

AIC(fit5_tmb, fit_simple_re)

# We see that AIC is lower if we keep nWBV as a random effect.

AIC(fit_age_poly, fit_nwbv_poly_raw)

# AIC is lower if we have an nWBV squared term than if we have an age squared term.

AIC(fit_nwbv_poly_raw, fit5_tmb)

# AIC is lower if we incorporate the squared nWBV term into our linear model.

AIC(fit_age_poly, fit5_tmb)

# AIC is also lower if we incorpoarte the squared age term into our linear model.

# What if we incorporate both squared age and squared nWBV?

fit_both <- glmmTMB(
  MMSE ~ nWBV + nWBV_sq + Age + Age_sq + SES + nWBV:SES + nWBV_sq:SES + Age:SES + (1 + nWBV | Subject.ID),
  data = dc,
  REML = TRUE
)

sim_both <- simulateResiduals(fit_both)
plot(sim_both)

# We see that our residuals are even more poorly behaved.

AIC(fit_both, fit_nwbv_poly_raw)

# Moreover, our AIC is worse than if we just have a squared nWBV term.

# Let's add the squared nWBV term to our LME.

test <- lme(MMSE ~ nWBV + nWBV_sq + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

test_ml <- update(test, method = "ML")
fit5_ml <- update(fit5, method = "ML")
anova(test_ml, fit5_ml)

# Note that all terms are still statistically significant (p-values < 0.02) in this new model. This model performs slightly worse # than fit5 in terms of AIC, but correlation between random slope and intercept is more stable (-0.961).


test <- lme(MMSE ~ nWBV + nWBV_sq + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

# Again, what if we drop nWBV from the random effects?

test_ml <- update(test, method = "ML")
anova(fit5_ml, test_ml)

# Dropping nWBV from random effects significantly reduces performance of this new model.

# What happens if we add squared age to our LME?

test <- lme(MMSE ~ nWBV + Age_sq + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

test_ml <- update(test, method = "ML")
anova(fit5_ml, test_ml)

# Squared age also sees improvement in correlation and performs slightly worse than fit5; not as much improvement as squared 
# nWBV.

#We will keep the model that includes squared nWBV; call it fit6.

fit6 <- lme(MMSE ~ nWBV + nWBV_sq + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + nWBV | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(fit6)

#What if we do within subject-centering for nWBV?

fit7 <- lme(MMSE ~ nWBV + nWBV_sq + Age + SES + nWBV * SES + Age * SES, 
            data = dc, 
            random = ~ 1 + dvar(nWBV, Subject.ID) | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(fit7)

# Correlation improves again, but Age:SES becomes insignificant. Can we drop it?

test <- lme(MMSE ~ nWBV + nWBV_sq + Age + SES + nWBV * SES, 
            data = dc, 
            random = ~ 1 + dvar(nWBV, Subject.ID) | Subject.ID,
            control = lmeControl(opt = "optim", maxIter = 200, msMaxIter = 200))
summary(test)

ml_test <- update(test, method = "ML")
ml_fit7 <- update(fit7, method = "ML")
anova(ml_test, ml_fit7)

# Although the correlation improves, we again see that the model has a relatively higher AIC, and we also see that
# squared nWBV and nWBV * SES becoth become statistically insignificant (p-values > 0.05). Centering seems to be the wrong
# choice here. What happens if we look at the residual plots?

fit7_tmb <- glmmTMB(
  MMSE ~ nWBV + nWBV_sq + Age + SES + nWBV * SES + Age * SES + 
  (1 + dvar(nWBV, Subject.ID) | Subject.ID),
  data = dc,
  REML = TRUE
)

res7 <- simulateResiduals(fittedModel = fit7_tmb)
plot(res7)

# Residuals worsen significantly with centered nWBV.

# We have three models to consider so far; fit5, fit6, fit7. Let's do an ANOVA test.

ml_fit5 <- update(fit5, method = "ML")
ml_fit6 <- update(fit6, method = "ML")
ml_fit7 <- update(fit7, method = "ML")
anova(ml_fit5, ml_fit6, ml_fit7)

# We see that our linear model, fit5, has the lowest AIC, closely followed by our squared nWBV model, fit6. Just as earlier,
# centering lowers AIC.

# Since non-linearity shows improvement, let's incorporate splines.

dc$ns_nWBV <- ns(dc$nWBV, df = 4)
ns_basis <- ns(dc$nWBV, df = 4)
ns_df <- as.data.frame(ns_basis)
colnames(ns_df) <- paste0("ns_nWBV_", 1:ncol(ns_df))

dc <- bind_cols(dc, ns_df)

fit_spline <- lme(MMSE ~ ns_nWBV_1 + ns_nWBV_2 + ns_nWBV_3 + Age + SES + nWBV * SES + Age * SES,
  random = ~ 1 + nWBV | Subject.ID,
  data = dc,
  method = "REML"
)

summary(fit_spline)

# We see further improvement in the random effects correlation. Note that we have omitted the fourth basis function from the
# model as it is collinear with the others.

# What happens if we look at residuals for our spline model?

fit_spline_tmb <- glmmTMB(
  MMSE ~ ns_nWBV_1 + ns_nWBV_2 + ns_nWBV_3 + Age + SES + nWBV * SES + Age * SES
  + (1 + nWBV | Subject.ID),
  data = dc,
  REML = TRUE
)

res_spline <- simulateResiduals(fit_spline_tmb)
plot(res_spline)

# We see the same issues in residuals as with our earlier models.
```

# Possible Solutions for Residuals

At this point, it is reasonable to conclude that the issues with residuals arise from the data being highly non-normal; this gives us an idea of where the limitations in LMEs applied to non-normal data arise.

A possible solution is to instead construct a glmmTMB and then use its zero-inflation feature. Most of the MMSE scores are at or very close to 30, so we can reverse the ordering to have most scores be at or close to 0 and then account for zero-inflation.

As a simple example, let's apply this approach to fit5 (note that we omit the random slope to resolve convergence issues:

```{r}
dc_c <- dc
dc_c$MMSE <- 30 - dc_c$MMSE
dc_c$nWBV_scaled <- scale(dc_c$nWBV)[,1]
dc_c$Age_scaled <- scale(dc_c$Age)[,1]
dc_c$SES_scaled <- scale(dc_c$SES)[,1]

fit5_tmb <- glmmTMB(
  MMSE ~ nWBV_scaled + Age_scaled + SES_scaled + 
         nWBV_scaled * SES_scaled + Age_scaled * SES_scaled + 
         (1 | Subject.ID),
  data = dc_c,
  ziformula = ~ Age_scaled,
  family = poisson,
  REML = TRUE
)

AIC(fit5, fit5_tmb)
```

We immediately see a significantly lower AIC. Moreover, we can plot the residuals:

```{r}
res1 <- simulateResiduals(fit5_tmb)
plot(res1)
```

We immediately see improvement in the residual plots; the curves are relatively flatter, and this model now passes the KS test and outlier test, indicating that the residuals are uniformly distributed and the model is less affected by outliers.

It was my intention to further explore this approach for fit6 and fit_spline, but I ran into convergence issues that I was unable to resolve due to time constraints for the project. However, from the improvement in this example, I believe this approach to be very promising.

# Model Comparison

At this point we have three models to consider; fit5, fit6, and fit_spline. Let's do an ANOVA test:

```{r}
ml_fit5 <- update(fit5, method = "ML")
ml_fit6 <- update(fit6, method = "ML")
ml_fit_spline <- update(fit_spline, method = "ML")
anova(ml_fit5, ml_fit6, ml_fit_spline)
```

We see that fit_spline has the lowest AIC, followed closely by fit5. The low p-value (< 0.02) for fit_spline suggests that the extra terms provide a statistically significant improvement to model fit.

Let's compare the mean slopes of the trajectories generated by these models to the mean slopes of the actual trajectories that we computed earlier.

Here are the slopes we originally computed:

```{r}
print(avg_slopes_by_CDR_old)
```

Now we compute the slopes generated by these models.

For fit5:
```{r}
dc$fit_vals5 <- fitted(fit5)

dc_slope <- dc %>%
  select(Subject.ID, Age, fit_vals5, CDR) %>%
  group_by(Subject.ID, CDR) %>%
  arrange(Age, .by_group = TRUE) %>%
  summarise(
    slope = if (n() >= 2) coef(lm(fit_vals5 ~ Age))[2] else NA_real_,
    .groups = "drop"
  )

avg_slopes_by_CDR_5 <- dc_slope %>%
  group_by(CDR) %>%
  summarise(
    avg_slope = mean(slope, na.rm = TRUE),
    n = n()
  )

print(avg_slopes_by_CDR_5)
```

For fit6:
```{r}
dc$fit_vals6 <- fitted(fit6)

dc_slope <- dc %>%
  select(Subject.ID, Age, fit_vals6, CDR) %>%
  group_by(Subject.ID, CDR) %>%
  arrange(Age, .by_group = TRUE) %>%
  summarise(
    slope = if (n() >= 2) coef(lm(fit_vals6 ~ Age))[2] else NA_real_,
    .groups = "drop"
  )

avg_slopes_by_CDR_6 <- dc_slope %>%
  group_by(CDR) %>%
  summarise(
    avg_slope = mean(slope, na.rm = TRUE),
    n = n()
  )

print(avg_slopes_by_CDR_6)
```

For fit_spline:
```{r}
dc$fit_spline <- fitted(fit_spline)

dc_slope <- dc %>%
  select(Subject.ID, Age, fit_spline, CDR) %>%
  group_by(Subject.ID, CDR) %>%
  arrange(Age, .by_group = TRUE) %>%
  summarise(
    slope = if (n() >= 2) coef(lm(fit_spline ~ Age))[2] else NA_real_,
    .groups = "drop"
  )

avg_slopes_by_CDR <- dc_slope %>%
  group_by(CDR) %>%
  summarise(
    avg_slope = mean(slope, na.rm = TRUE),
    n = n()
  )

print(avg_slopes_by_CDR)
```

We see that fit5 has the closest mean slope for CDR = 1 to the actual mean trajectory slope while fit_spline has the closest mean slopes for CDR = 0 and CDR = 0.5. Note that the mean slope for fit5 is positive, suggesting an average increase in MMSE for those with CDR = 0; we would expect MMSE to decrease with age, even for those with CDR = 0. Note also that some subjects were lost in attempting to compute the mean slopes for the actual trajectories, so these estimates are limited compared to our model estimates, particularly for CDR = 1 which only has 9 subjects.

# Model Interpretation

Finally, we interpret the fixed and random effects of our models.

For fit5:

```{r}
summary(fit5)
```

We will not attempt to interpret random effects here since the correlation is very close to -1.

For the fixed effects, the coefficient of nWBV (~85.40) suggests that increased nWBV is associated with increased MMSE, and this effect is dampened by an interaction with SES, as nWBV * SES has coefficient (~-21.27). The coefficient of Age (~0.25) is associated with increased MMSE, but this effect is dampened by an interaction with SES, as Age * SES has coefficient (~-0.08). The coefficient of SES is ~6.48, suggesting that higher SES is associated with higher MMSE. These coefficients are all statistically significant (p-values < 0.01).

For fit6:

```{r}
summary(fit6)
```

For random effects, the correlation between the slope term nWBV and intercept is -0.961; this suggests that those with a higher baseline MMSE experience a greater decline in MMSE with loss of brain volume. 

For the fixed effects, the coefficients in fit6 that are present in fit5 are all similar, and are all to be interpreted similarly, except for the additional squared nWBV coefficient (~-239.25), which is suggestive of a non-linear relationship between nWBV and MMSE, and that there are diminishing returns in MMSE as nWBV increases.

For fit_spline:

```{r}
summary(fit_spline)
```

For random effects, the interpretation is the same as in fit6.

For fixed effects, the coefficients in fit_spline that are present in fit5 are again similar and are again to be interpreted similarly. The coefficients for the spline terms (~4.30, ~1.62, ~8.34) are suggestive of a non-linear relationship between nWBV and MMSE, though no further conclusions can be drawn.

# On the use of AI

ChatGPT and Claude were very useful for many aspects of this project, and I again want to give credit to both LLMs for their assistance. I have outlined the details of their use in the Discussion section of the report.